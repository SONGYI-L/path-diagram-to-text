{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def preprocess_data(json_data):\n",
    "    processed_data = []\n",
    "\n",
    "    for sample in json_data:\n",
    "        sample_id = sample[\"id\"]\n",
    "        kbs = sample[\"kbs\"]\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        input_sequence = \"\"\n",
    "        for kb_entry in kbs.values():\n",
    "          obj = kb_entry[0]\n",
    "          subject = kb_entry[2][0][1]\n",
    "          predicate = kb_entry[2][0][0]\n",
    "\n",
    "          input_text = subject + \",\" + predicate + \",\" + obj\n",
    "          input_sequence += input_text + \",\"\n",
    "\n",
    "        for sentence in text:\n",
    "            processed_data.append({\"id\": sample_id, \"input\": input_sequence.strip(), \"target\": sentence})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"C:/Users/lsong/Documents/bart/train.json\", \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(\"C:/Users/lsong/Documents/bart/val.json\", \"r\") as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "\n",
    "#batch_size = 100  # 每批处理的样本数量\n",
    "#num_samples = len(processed_data)\n",
    "\n",
    "#for i in range(0, num_samples, batch_size):\n",
    "#    batch = processed_data[i:i+batch_size]\n",
    "#    print(batch)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/lsong/Documents/bart/fintune_model\",  # 保存微调模型的路径\n",
    "    num_train_epochs=30,  # 训练轮数\n",
    "    per_device_train_batch_size=8,  # 批量大小\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    logging_dir=\"C:/Users/lsong/Documents/bart/finetune_logs\",  # 日志保存路径\n",
    "    logging_steps=100,  # 每隔多少步打印一次日志\n",
    ")\n",
    "\n",
    "# 加载预训练BART模型和tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 准备训练数据（假设你的训练数据已经预处理为input和target）\n",
    "train_data = preprocess_data(train_data)\n",
    "val_data = preprocess_data(val_data)\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # Set the maximum sequence length\n",
    "    max_length = 512\n",
    "\n",
    "    # Tokenize and pad the inputs\n",
    "    inputs_encodings = tokenizer.batch_encode_plus(inputs, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Tokenize and pad the targets\n",
    "    targets_encodings = tokenizer.batch_encode_plus(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    inputs_encodings = {k: torch.tensor(v) for k, v in inputs_encodings.items()}\n",
    "    targets_encodings = {k: torch.tensor(v) for k, v in targets_encodings.items()}\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': inputs_encodings['input_ids'],\n",
    "        'attention_mask': inputs_encodings['attention_mask'],\n",
    "        'labels': targets_encodings['input_ids'],\n",
    "    }\n",
    "    return encodings\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "train_data = Dataset.from_dict(train_df).map(preprocess_function, batched=True)\n",
    "val_data = Dataset.from_dict(val_df).map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# 创建Trainer对象并执行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def preprocess_new_data(json_data):\n",
    "    input_data = []\n",
    "\n",
    "    for data_item in json_data:\n",
    "        if \"object\" in data_item:\n",
    "            data_id = int(data_item[\"id\"])  # Convert the 'id' to an integer\n",
    "            if isinstance(data_item[\"object\"], list):\n",
    "                object_list = data_item[\"object\"]\n",
    "            else:\n",
    "                object_list = [data_item[\"object\"]]\n",
    "            target_text = data_item[\"text\"]\n",
    "\n",
    "            input_text_list = []\n",
    "            for obj in object_list:\n",
    "                input_text_list.append(\",\".join(obj[\"name\"]))\n",
    "            \n",
    "            input_text = \",\".join(input_text_list)\n",
    "\n",
    "            input_data.append({\"id\": data_id, \"input\": input_text, \"target\": target_text})\n",
    "\n",
    "    return input_data\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/train.json\", \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "    \n",
    "train_data = preprocess_new_data(train_data)\n",
    "#val_data = preprocess_data(val_data)\n",
    "\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \n",
      " 99%|█████████▉| 257300/259620 [118:21:19<1:00:50,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 1.856559587088822e-07, 'epoch': 29.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \n",
      " 99%|█████████▉| 257400/259620 [118:23:58<59:40,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 1.7795239195747632e-07, 'epoch': 29.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 257500/259620 [118:26:37<55:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 1.7024882520607044e-07, 'epoch': 29.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \n",
      " 99%|█████████▉| 257600/259620 [118:29:17<53:25,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'learning_rate': 1.6254525845466454e-07, 'epoch': 29.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 257700/259620 [118:31:56<51:28,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'learning_rate': 1.5491872737077269e-07, 'epoch': 29.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 257800/259620 [118:34:34<48:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 1.4721516061936678e-07, 'epoch': 29.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 257900/259620 [118:37:13<46:02,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 1.3951159386796087e-07, 'epoch': 29.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 258000/259620 [118:39:50<41:41,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0065, 'learning_rate': 1.3180802711655497e-07, 'epoch': 29.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 258100/259620 [118:42:29<39:27,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 1.2410446036514906e-07, 'epoch': 29.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 258200/259620 [118:45:07<36:47,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'learning_rate': 1.1640089361374318e-07, 'epoch': 29.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      " 99%|█████████▉| 258300/259620 [118:47:39<33:01,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 1.0869732686233727e-07, 'epoch': 29.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258400/259620 [118:50:11<30:57,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 1.0099376011093137e-07, 'epoch': 29.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258500/259620 [118:52:43<28:14,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 9.329019335952547e-08, 'epoch': 29.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258600/259620 [118:55:17<26:13,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0055, 'learning_rate': 8.558662660811957e-08, 'epoch': 29.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258700/259620 [118:57:49<23:23,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'learning_rate': 7.788305985671366e-08, 'epoch': 29.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258800/259620 [119:00:21<20:49,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'learning_rate': 7.017949310530777e-08, 'epoch': 29.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 258900/259620 [119:02:53<18:09,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 6.247592635390186e-08, 'epoch': 29.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259000/259620 [119:05:26<15:44,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0062, 'learning_rate': 5.477235960249596e-08, 'epoch': 29.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259100/259620 [119:08:01<13:02,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 4.706879285109006e-08, 'epoch': 29.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259200/259620 [119:10:32<10:41,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0055, 'learning_rate': 3.936522609968416e-08, 'epoch': 29.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259300/259620 [119:13:04<08:07,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 3.166165934827825e-08, 'epoch': 29.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259400/259620 [119:15:36<05:31,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0053, 'learning_rate': 2.3958092596872356e-08, 'epoch': 29.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259500/259620 [119:18:08<03:01,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'learning_rate': 1.6254525845466452e-08, 'epoch': 29.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|█████████▉| 259600/259620 [119:20:42<00:30,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.007, 'learning_rate': 8.55095909406055e-09, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \n",
      "100%|██████████| 259620/259620 [119:21:13<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 429673.1445, 'train_samples_per_second': 2.417, 'train_steps_per_second': 0.604, 'train_loss': 0.014765281499456105, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=259620, training_loss=0.014765281499456105, metrics={'train_runtime': 429673.1445, 'train_samples_per_second': 2.417, 'train_steps_per_second': 0.604, 'train_loss': 0.014765281499456105, 'epoch': 30.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 加载预训练BART模型和tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # Set the maximum sequence length\n",
    "    max_length = 1024\n",
    "\n",
    "    # Tokenize and pad the inputs\n",
    "    inputs_encodings = tokenizer.batch_encode_plus(inputs, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Tokenize and pad the targets\n",
    "    targets_encodings = tokenizer.batch_encode_plus(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    inputs_encodings = {k: torch.tensor(v) for k, v in inputs_encodings.items()}\n",
    "    targets_encodings = {k: torch.tensor(v) for k, v in targets_encodings.items()}\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': inputs_encodings['input_ids'],\n",
    "        'attention_mask': inputs_encodings['attention_mask'],\n",
    "        'labels': targets_encodings['input_ids'],\n",
    "    }\n",
    "    return encodings\n",
    "\n",
    "def preprocess_data(json_data):\n",
    "    processed_data = []\n",
    "\n",
    "    for sample in json_data:\n",
    "        sample_id = sample[\"id\"]\n",
    "        kbs = sample[\"kbs\"]\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        input_sequence = \"\"\n",
    "        for kb_entry in kbs.values():\n",
    "          obj = kb_entry[0]\n",
    "          subject = kb_entry[2][0][1]\n",
    "          predicate = kb_entry[2][0][0]\n",
    "\n",
    "          input_text = subject + \",\" + predicate + \",\" + obj\n",
    "          input_sequence += input_text + \",\"\n",
    "\n",
    "        for sentence in text:\n",
    "            processed_data.append({\"id\": sample_id, \"input\": input_sequence.strip(), \"target\": sentence})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"C:/Users/lsong/Documents/bart/train.json\", \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(\"C:/Users/lsong/Documents/bart/val.json\", \"r\") as file:\n",
    "    val_data = json.load(file)\n",
    "# 准备训练数据（假设你的训练数据已经预处理为input和target）\n",
    "train_data = preprocess_data(train_data)\n",
    "val_data = preprocess_data(val_data)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "train_data = Dataset.from_dict(train_df).map(preprocess_function, batched=True)\n",
    "val_data = Dataset.from_dict(val_df).map(preprocess_function, batched=True)\n",
    "    \n",
    "def preprocess_new_data(json_data):\n",
    "    input_data = []\n",
    "\n",
    "    for data_item in json_data:\n",
    "        if \"object\" in data_item:\n",
    "            data_id = int(data_item[\"id\"])  # Convert the 'id' to an integer\n",
    "            if isinstance(data_item[\"object\"], list):\n",
    "                object_list = data_item[\"object\"]\n",
    "            else:\n",
    "                object_list = [data_item[\"object\"]]\n",
    "            target_text = data_item[\"text\"]\n",
    "\n",
    "            input_text_list = []\n",
    "            for obj in object_list:\n",
    "                input_text_list.append(\",\".join(obj[\"name\"]))\n",
    "            \n",
    "            input_text = \",\".join(input_text_list)\n",
    "\n",
    "            input_data.append({\"id\": data_id, \"input\": input_text, \"target\": target_text})\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Load the new JSON data\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/train.json\", \"r\") as file:\n",
    "    new_train_data = json.load(file)\n",
    "\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/val.json\", \"r\") as file:\n",
    "    new_val_data = json.load(file)\n",
    "    \n",
    "new_train_data = preprocess_new_data(new_train_data)\n",
    "new_val_data = preprocess_new_data(new_val_data)\n",
    "\n",
    "new_train_df = pd.DataFrame(new_train_data)\n",
    "new_val_df = pd.DataFrame(new_val_data)\n",
    "\n",
    "new_train_dataset = Dataset.from_dict(new_train_df).map(preprocess_function, batched=True)\n",
    "new_val_dataset = Dataset.from_dict(new_val_df).map(preprocess_function, batched=True)\n",
    "\n",
    "combined_train_dataset = concatenate_datasets([train_data, new_train_dataset])\n",
    "combined_val_dataset = concatenate_datasets([val_data, new_val_dataset])\n",
    "\n",
    "#combined_train_data = train_data[\"id\"] + new_train_dataset[\"id\"]\n",
    "#combined_train_input_data = train_data[\"input\"] + new_train_dataset[\"input\"]\n",
    "#combined_train_target_data = train_data[\"target\"] + new_train_dataset[\"target\"]\n",
    "\n",
    "#combined_train_dataset = Dataset.from_dict({\"id\": combined_train_data, \"input\": combined_train_input_data, \"target\": combined_train_target_data})\n",
    "\n",
    "#combined_val_data = val_data[\"id\"] + new_val_dataset[\"id\"]\n",
    "#combined_val_input_data = val_data[\"input\"] + new_val_dataset[\"input\"]\n",
    "#combined_val_target_data = val_data[\"target\"] + new_val_dataset[\"target\"]\n",
    "\n",
    "#combined_val_dataset = Dataset.from_dict({\"id\": combined_val_data, \"input\": combined_val_input_data, \"target\": combined_val_target_data})\n",
    "\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/lsong/Documents/bart/fintune_model/web_sem\",  # 保存微调模型的路径\n",
    "    num_train_epochs=30,  # 训练轮数\n",
    "    per_device_train_batch_size=4,  # 批量大小\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    logging_dir=\"C:/Users/lsong/Documents/bart/finetune_logs/web_sem\",  # 日志保存路径\n",
    "    logging_steps=100,  # 每隔多少步打印一次日志\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# 创建Trainer对象并执行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=combined_val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lsong\\anaconda3\\envs\\bart\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  5%|▍         | 100/2100 [02:00<40:18,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6173, 'learning_rate': 1.904761904761905e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 200/2100 [04:00<38:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4948, 'learning_rate': 1.8095238095238097e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 300/2100 [05:59<36:16,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4548, 'learning_rate': 1.7142857142857142e-05, 'epoch': 4.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 400/2100 [07:59<34:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3965, 'learning_rate': 1.6190476190476193e-05, 'epoch': 5.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 500/2100 [09:58<32:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3924, 'learning_rate': 1.523809523809524e-05, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 600/2100 [12:00<30:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3624, 'learning_rate': 1.4285714285714287e-05, 'epoch': 8.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 700/2100 [13:58<20:49,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3374, 'learning_rate': 1.3333333333333333e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 800/2100 [15:58<26:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3282, 'learning_rate': 1.2380952380952383e-05, 'epoch': 11.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 900/2100 [17:58<24:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2989, 'learning_rate': 1.1428571428571429e-05, 'epoch': 12.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1000/2100 [19:57<22:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2885, 'learning_rate': 1.0476190476190477e-05, 'epoch': 14.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1100/2100 [21:58<20:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2875, 'learning_rate': 9.523809523809525e-06, 'epoch': 15.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1200/2100 [23:57<17:56,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2663, 'learning_rate': 8.571428571428571e-06, 'epoch': 17.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1300/2100 [25:57<16:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2659, 'learning_rate': 7.61904761904762e-06, 'epoch': 18.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1400/2100 [27:55<10:25,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2488, 'learning_rate': 6.666666666666667e-06, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 1500/2100 [29:55<12:53,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.251, 'learning_rate': 5.7142857142857145e-06, 'epoch': 21.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1600/2100 [31:56<09:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2313, 'learning_rate': 4.761904761904762e-06, 'epoch': 22.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1700/2100 [33:54<07:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2412, 'learning_rate': 3.80952380952381e-06, 'epoch': 24.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1800/2100 [35:53<05:59,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.232, 'learning_rate': 2.8571428571428573e-06, 'epoch': 25.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1900/2100 [37:51<03:58,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2168, 'learning_rate': 1.904761904761905e-06, 'epoch': 27.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2000/2100 [39:50<02:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2224, 'learning_rate': 9.523809523809525e-07, 'epoch': 28.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [41:49<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2219, 'learning_rate': 0.0, 'epoch': 30.0}\n",
      "{'train_runtime': 2509.6344, 'train_samples_per_second': 3.323, 'train_steps_per_second': 0.837, 'train_loss': 0.3169720340910412, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2100, training_loss=0.3169720340910412, metrics={'train_runtime': 2509.6344, 'train_samples_per_second': 3.323, 'train_steps_per_second': 0.837, 'train_loss': 0.3169720340910412, 'epoch': 30.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 加载预训练BART模型和tokenizer\n",
    "model_name = \"C:/Users/lsong/Documents/bart/fintune_model/web/checkpoint-64000\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # Set the maximum sequence length\n",
    "    max_length = 1024\n",
    "\n",
    "    # Tokenize and pad the inputs\n",
    "    inputs_encodings = tokenizer.batch_encode_plus(inputs, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Tokenize and pad the targets\n",
    "    targets_encodings = tokenizer.batch_encode_plus(targets, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "    # Convert the lists to tensors\n",
    "    inputs_encodings = {k: torch.tensor(v) for k, v in inputs_encodings.items()}\n",
    "    targets_encodings = {k: torch.tensor(v) for k, v in targets_encodings.items()}\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': inputs_encodings['input_ids'],\n",
    "        'attention_mask': inputs_encodings['attention_mask'],\n",
    "        'labels': targets_encodings['input_ids'],\n",
    "    }\n",
    "    return encodings\n",
    "    \n",
    "def preprocess_new_data(json_data):\n",
    "    input_data = []\n",
    "\n",
    "    for data_item in json_data:\n",
    "        if \"object\" in data_item:\n",
    "            data_id = int(data_item[\"id\"])  # Convert the 'id' to an integer\n",
    "            if isinstance(data_item[\"object\"], list):\n",
    "                object_list = data_item[\"object\"]\n",
    "            else:\n",
    "                object_list = [data_item[\"object\"]]\n",
    "            target_text = data_item[\"text\"]\n",
    "\n",
    "            input_text_list = []\n",
    "            for obj in object_list:\n",
    "                input_text_list.append(\",\".join(obj[\"name\"]))\n",
    "            \n",
    "            input_text = \",\".join(input_text_list)\n",
    "\n",
    "            input_data.append({\"id\": data_id, \"input\": input_text, \"target\": target_text})\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Load the new JSON data\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/train.json\", \"r\") as file:\n",
    "    new_train_data = json.load(file)\n",
    "\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/val.json\", \"r\") as file:\n",
    "    new_val_data = json.load(file)\n",
    "    \n",
    "new_train_data = preprocess_new_data(new_train_data)\n",
    "new_val_data = preprocess_new_data(new_val_data)\n",
    "\n",
    "new_train_df = pd.DataFrame(new_train_data)\n",
    "new_val_df = pd.DataFrame(new_val_data)\n",
    "\n",
    "new_train_dataset = Dataset.from_dict(new_train_df).map(preprocess_function, batched=True)\n",
    "new_val_dataset = Dataset.from_dict(new_val_df).map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/lsong/Documents/bart/fintune_model/sem_on_web\",  # 保存微调模型的路径\n",
    "    num_train_epochs=30,  # 训练轮数\n",
    "    per_device_train_batch_size=4,  # 批量大小\n",
    "    learning_rate=2e-5,  # 学习率\n",
    "    weight_decay=0.01,  # 权重衰减\n",
    "    logging_dir=\"C:/Users/lsong/Documents/bart/finetune_logs/sem_on_web\",  # 日志保存路径\n",
    "    logging_steps=100,  # 每隔多少步打印一次日志\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# 创建Trainer对象并执行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=new_train_dataset,\n",
    "    eval_dataset=new_val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lsong\\anaconda3\\envs\\bart\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成结果已保存到文件: C:/Users/lsong/Documents/bart/sem/web_fintune_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"C:/Users/lsong/Documents/bart/fintune_model/web/checkpoint-64000\"  # 或者是您微调后保存的模型路径\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(json_data):\n",
    "    processed_data = []\n",
    "\n",
    "    for sample in json_data:\n",
    "        sample_id = sample[\"id\"]\n",
    "        kbs = sample[\"kbs\"]\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        input_sequence = \"\"\n",
    "        for kb_entry in kbs.values():\n",
    "            obj = kb_entry[0]\n",
    "            subject = kb_entry[2][0][1]\n",
    "            predicate = kb_entry[2][0][0]\n",
    "\n",
    "            input_text = subject + \",\" + predicate + \",\" + obj\n",
    "            input_sequence += input_text + \",\"\n",
    "\n",
    "        for sentence in text:\n",
    "            processed_data.append({\"id\": sample_id, \"input\": input_sequence.strip(), \"target\": sentence})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def preprocess_new_data(json_data):\n",
    "    input_data = []\n",
    "\n",
    "    for data_item in json_data:\n",
    "        if \"object\" in data_item:\n",
    "            data_id = int(data_item[\"id\"])  # Convert the 'id' to an integer\n",
    "            if isinstance(data_item[\"object\"], list):\n",
    "                object_list = data_item[\"object\"]\n",
    "            else:\n",
    "                object_list = [data_item[\"object\"]]\n",
    "            target_text = data_item[\"text\"]\n",
    "\n",
    "            input_text_list = []\n",
    "            for obj in object_list:\n",
    "                input_text_list.append(\",\".join(obj[\"name\"]))\n",
    "            \n",
    "            input_text = \",\".join(input_text_list)\n",
    "\n",
    "            input_data.append({\"id\": data_id, \"input\": input_text, \"target\": target_text})\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/test.json\", \"r\") as f:\n",
    "    input_data = json.load(f)\n",
    "    \n",
    "processed_data = preprocess_new_data(input_data)\n",
    "\n",
    "generated_entries = []  # 存储生成的结果\n",
    "\n",
    "for entry in processed_data:\n",
    "    sample_id = entry[\"id\"]\n",
    "    input_text = entry[\"input\"]\n",
    "\n",
    "    # 将逗号分隔的三元组转换为字符串列表\n",
    "    input_triplets = input_text.split(\",\")\n",
    "\n",
    "    # 使用BART tokenizer将文本转换为token的索引形式\n",
    "    input_ids = tokenizer.encode(input_triplets, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # 生成文本\n",
    "    output_ids = model.generate(input_ids, max_length=1000, num_beams=5, no_repeat_ngram_size=2)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 将生成结果添加到列表中\n",
    "    generated_entry = {\n",
    "        \"id\": sample_id,\n",
    "        \"input_text\": input_text,\n",
    "        \"generated_text\": output_text\n",
    "    }\n",
    "    generated_entries.append(generated_entry)\n",
    "\n",
    "# 将生成的结果保存为JSON文件\n",
    "output_file = \"C:/Users/lsong/Documents/bart/sem/web_fintune_output.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(generated_entries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"生成结果已保存到文件:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成结果已保存到文件: C:/Users/lsong/Documents/bart/sem/nofintune_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/bart-base\"  # 或者是您微调后保存的模型路径\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(json_data):\n",
    "    processed_data = []\n",
    "\n",
    "    for sample in json_data:\n",
    "        sample_id = sample[\"id\"]\n",
    "        kbs = sample[\"kbs\"]\n",
    "        text = sample[\"text\"]\n",
    "\n",
    "        input_sequence = \"\"\n",
    "        for kb_entry in kbs.values():\n",
    "            obj = kb_entry[0]\n",
    "            subject = kb_entry[2][0][1]\n",
    "            predicate = kb_entry[2][0][0]\n",
    "\n",
    "            input_text = subject + \",\" + predicate + \",\" + obj\n",
    "            input_sequence += input_text + \",\"\n",
    "\n",
    "        for sentence in text:\n",
    "            processed_data.append({\"id\": sample_id, \"input\": input_sequence.strip(), \"target\": sentence})\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def preprocess_new_data(json_data):\n",
    "    input_data = []\n",
    "\n",
    "    for data_item in json_data:\n",
    "        if \"object\" in data_item:\n",
    "            data_id = int(data_item[\"id\"])  # Convert the 'id' to an integer\n",
    "            if isinstance(data_item[\"object\"], list):\n",
    "                object_list = data_item[\"object\"]\n",
    "            else:\n",
    "                object_list = [data_item[\"object\"]]\n",
    "            target_text = data_item[\"text\"]\n",
    "\n",
    "            input_text_list = []\n",
    "            for obj in object_list:\n",
    "                input_text_list.append(\",\".join(obj[\"name\"]))\n",
    "            \n",
    "            input_text = \",\".join(input_text_list)\n",
    "\n",
    "            input_data.append({\"id\": data_id, \"input\": input_text, \"target\": target_text})\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(\"C:/Users/lsong/Documents/bart/sem/test.json\", \"r\") as f:\n",
    "    input_data = json.load(f)\n",
    "    \n",
    "processed_data = preprocess_new_data(input_data)\n",
    "\n",
    "generated_entries = []  # 存储生成的结果\n",
    "\n",
    "for entry in processed_data:\n",
    "    sample_id = entry[\"id\"]\n",
    "    input_text = entry[\"input\"]\n",
    "\n",
    "    # 将逗号分隔的三元组转换为字符串列表\n",
    "    input_triplets = input_text.split(\",\")\n",
    "\n",
    "    # 使用BART tokenizer将文本转换为token的索引形式\n",
    "    input_ids = tokenizer.encode(input_triplets, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # 生成文本\n",
    "    output_ids = model.generate(input_ids, max_length=1000, num_beams=5, no_repeat_ngram_size=2)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # 将生成结果添加到列表中\n",
    "    generated_entry = {\n",
    "        \"id\": sample_id,\n",
    "        \"input_text\": input_text,\n",
    "        \"generated_text\": output_text\n",
    "    }\n",
    "    generated_entries.append(generated_entry)\n",
    "\n",
    "# 将生成的结果保存为JSON文件\n",
    "output_file = \"C:/Users/lsong/Documents/bart/sem/nofintune_output.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(generated_entries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"生成结果已保存到文件:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成.target文件: C:/Users/lsong/Documents/bart/sem/images1.target\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取Excel文件\n",
    "excel_file = \"C:/Users/lsong/Documents/bart/sem/images1.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# 提取\"textization\"列中的文本内容，并按要求处理成目标格式\n",
    "target_text = \"\\n\\n\".join(df[\"textization\"])\n",
    "\n",
    "# 保存为.target文件\n",
    "output_file = \"C:/Users/lsong/Documents/bart/sem/images1.target\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(target_text)\n",
    "\n",
    "print(\"已生成.target文件:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to C:/Users/lsong/Documents/bart/sem/images1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Read Excel file\n",
    "excel_file = 'C:/Users/lsong/Documents/bart/sem/images.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Read JSON file\n",
    "json_file = 'C:/Users/lsong/Documents/bart/sem/fintune_output.json'\n",
    "with open(json_file, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Extract ids from JSON data\n",
    "json_ids = [item['id'] for item in json_data]\n",
    "\n",
    "def extract_numeric(image):\n",
    "    match = re.search(r'(\\d+)', image)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Apply the function to 'image' column and filter rows based on matching ids\n",
    "filtered_df = df[df['image'].apply(extract_numeric).isin(json_ids)]\n",
    "\n",
    "# Save filtered DataFrame to Excel\n",
    "output_file = 'C:/Users/lsong/Documents/bart/sem/images1.xlsx'\n",
    "filtered_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Filtered data saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('C:/Users/lsong/Documents/bart/sem/web_fintune_output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 创建字典来存储每个ID的第一个条目\n",
    "filtered_data = {}\n",
    "\n",
    "# 保留每个ID的第一个条目\n",
    "for item in data:\n",
    "    item['text'] = item.pop('generated_text')\n",
    "    \n",
    "    if item['id'] not in filtered_data:\n",
    "        filtered_data[item['id']] = item\n",
    "    \n",
    "    if 'input_text' in item:\n",
    "        del item['input_text']\n",
    "\n",
    "# 将字典转换回列表形式\n",
    "filtered_data = list(filtered_data.values())\n",
    "\n",
    "# 保存格式化输出的JSON文件\n",
    "with open('C:/Users/lsong/Documents/bart/sem/web_fintune_output.json', 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointgt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
